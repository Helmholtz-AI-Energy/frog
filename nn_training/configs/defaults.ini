[DEFAULT]
batch_size = 128
test_batch_size = 1024
input_shape = 3x32x32
num_classes = 10
data_augmentation = False

epochs = 200
seed = 0

dataset_root = data

output_base = results
output_name =
experiment_id =

# ---------- DATASETS -------------------------------------
[MNIST]
batch_size = 64
input_shape = 1x28x28

[CIFAR10]

[SVHN]

# ---------- GRADIENT COMPUTATION -------------------------
[BP]

[FG]
num_directions = 1
aggregation_mode = mean
tangent_sampler = normal
normalize_tangents = False
fg_computation_mode = sim
scaling_correction = False

[FROG]
num_directions = 1
aggregation_mode = orthogonal_projection
tangent_sampler = normal
normalize_tangents = False
fg_computation_mode = sim
scaling_correction = False

# ---------- MODELS ---------------------------------------
[FC]
hidden_size = 1024
num_hidden_layers = 2

[RESNET18]

[LENET5]

[VIT]
num_layers = 6
num_heads = 4
# Hidden dim = embedding dimension = split across the heads, default: 64 per Head
hidden_dim = 256
mlp_dim = 512
grouping_mode = wide_1
replicate_torchvision = True

[MLPMIXER]
hidden_dim = 512
token_mlp_dim = 256
channel_mlp_dim = 2048
num_blocks = 8
dropout = 0
grouping_mode = block_wise

# ---------- OPTIMIZERS -----------------------------------
[PLAIN_SGD]
optimizer_type = SGD
initial_lr = 0.1
lr_schedule = None

[SOTA_SGD]
optimizer_type = SGD
initial_lr = 0.1
lr_schedule = cosine
momentum = 0.9
weight_decay = 5e-4
nesterov = True

[PLAIN_ADAM]
optimizer_type = Adam
initial_lr = 0.1
weight_decay = 0.02
decoupled_weight_decay = True

[SOTA_ADAM]
optimizer_type = Adam
initial_lr = 0.001
lr_schedule = cosine
weight_decay = 0.02
decoupled_weight_decay = True